{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b77d07-2b1a-4479-9c13-a4efb788d963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.24.1)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: packaging in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (22.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: optree in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: rich in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d677fb-2add-4a45-b405-0c6c7e064af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist \n",
    "import numpy as np\n",
    "\n",
    "image_Path = \"A_Z Handwritten Data.csv\"\n",
    "#przygotwanie danych i etykiet w data,labels\n",
    "def load_az_dataset(image_Path):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for row in open(image_Path):\n",
    "        #podział w wierszach na etylietę i element\n",
    "        row=row.split(\",\")\n",
    "        label = int(row[0])\n",
    "        image = np.array([int(x) for x in row[1:]],dtype=\"uint8\")\n",
    "        image = image.reshape((28,28))\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "    data= np.array(data,dtype=\"float32\")\n",
    "    labels = np.array(labels,dtype=\"int\")\n",
    "    return data,labels\n",
    "def load_mnist_dataset():\n",
    "    ((trainData,trainLabels),(testData,testLabels))=mnist.load_data()\n",
    "    data = np.vstack([trainData,testData])\n",
    "    labels= np.hstack([trainLabels,testLabels])\n",
    "    return (data,labels)\n",
    "    # set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "    \n",
    "EPOCHS = 6\n",
    "INIT_LR = 1e-1\n",
    "BS=128\n",
    "(azData,azLabels) = load_az_dataset(image_Path)\n",
    "(digitsData,digitsLabels) = load_mnist_dataset()\n",
    "#dopisanie etypiet \n",
    "\n",
    "azLabels +=10\n",
    "data= np.vstack([azData,digitsData])\n",
    "labels = np.hstack([azLabels, digitsLabels])\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0\n",
    "\n",
    "# convertowanie opisu na wektor\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max() / classTotals[i]\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.20, stratify=labels, random_state=42)\n",
    "aug = ImageDataGenerator(rotation_range=10,zoom_range=0.05,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.15,horizontal_flip=False,fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24776ae4-58bf-413e-9338-01807c772525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "class ResNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, stages, filters, reg=0.0005):\n",
    "        input_shape = (height, width, depth)\n",
    "        input_tensor = Input(shape=input_shape)\n",
    "        \n",
    "        x = Conv2D(filters[0], (3, 3), padding='same', kernel_regularizer=l2(reg))(input_tensor)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        for i in range(len(stages)):\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "\n",
    "            x = ResNet.build_residual_block(x, filters[i + 1], stages[i], stride, reg=reg)\n",
    "\n",
    "        x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_residual_block(x, filters, blocks, stride=(1, 1), reg=0.0005):\n",
    "        shortcut = x\n",
    "\n",
    "        x = Conv2D(filters, (3, 3), strides=stride, padding='same', kernel_regularizer=l2(reg))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = Conv2D(filters, (3, 3), padding='same', kernel_regularizer=l2(reg))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if stride[0] != 1 or stride[1] != 1:\n",
    "            shortcut = Conv2D(filters, (1, 1), strides=stride, kernel_regularizer=l2(reg))(shortcut)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "        x = Add()([x, shortcut])\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            shortcut = x\n",
    "\n",
    "            x = Conv2D(filters, (3, 3), padding='same', kernel_regularizer=l2(reg))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            x = Conv2D(filters, (3, 3), padding='same', kernel_regularizer=l2(reg))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406145d9-2a41-4436-8083-45390fa1091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`keras.optimizers.legacy` is not supported in Keras 3. When using `tf.keras`, to continue using a `tf.keras.optimizers.legacy` optimizer, you can install the `tf_keras` package (Keras 2) and set the environment variable `TF_USE_LEGACY_KERAS=True` to configure TensorFlow to use `tf_keras` when accessing `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# initialize and compile our deep neural network\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] compiling model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_LR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_LR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation\n",
      "File \u001b[1;32mC:\\maszynowe_uczenie\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:115\u001b[0m, in \u001b[0;36mLegacyOptimizerWarning.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.optimizers.legacy` is not supported in Keras 3. When using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras`, to continue using a `tf.keras.optimizers.legacy` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer, you can install the `tf_keras` package (Keras 2) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset the environment variable `TF_USE_LEGACY_KERAS=True` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure TensorFlow to use `tf_keras` when accessing `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: `keras.optimizers.legacy` is not supported in Keras 3. When using `tf.keras`, to continue using a `tf.keras.optimizers.legacy` optimizer, you can install the `tf_keras` package (Keras 2) and set the environment variable `TF_USE_LEGACY_KERAS=True` to configure TensorFlow to use `tf_keras` when accessing `tf.keras`."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "# initialize and compile our deep neural network\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29269c9-8cfd-4ffc-8e77-ceec528249d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // 2200,\n",
    "\tepochs=EPOCHS,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)\n",
    "\n",
    "\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "# evaluate the network\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"Models/mymodel\", save_format=\"h5\")\n",
    "# construct a plot that plots and saves the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003ab70-7ddf-40cd-b6ce-b6c3933496d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // 1300,\n",
    "\tepochs=1,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)\n",
    "\n",
    "\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d091bd-124b-4185-a3b0-d6b323e9636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "#import wymaganych bibiotek pobranych wyżej\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "model = tf.keras.models.load_model('Models/mymodel')\n",
    "# initialize our list of output test images\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
    "\t# classify the character\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = labelNames[prediction[0]]\n",
    "    # extract the image from the test data and initialize the text\n",
    "    # label color as green (correct)\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "    # otherwise, the class label prediction is incorrect\n",
    "    if prediction[0] != np.argmax(testY[i]):\n",
    "        color = (0, 0, 255)\n",
    "    # merge the channels into one image, resize the image from 32x32\n",
    "    # to 96x96 so we can better see it and then draw the predicted\n",
    "    # label on the image\n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "        color, 2)\n",
    "    # add the image to our list of output images\n",
    "    images.append(image)\n",
    "# construct the montage for the images\n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "# show the output montage\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.imwrite(' montage.jpg',montage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891e078-6273-4d3e-a7b4-5f75c411aae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab769a45-be15-4789-b159-0eff7123221e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m img\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzadanie.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m imgCont\u001b[38;5;241m=\u001b[39m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[0;32m      6\u001b[0m imgFinal\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      9\u001b[0m imggray\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcvtColor(img,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=cv2.imread('zadanie.png')\n",
    "imgCont=img.copy()\n",
    "imgFinal=img.copy()\n",
    "\n",
    "\n",
    "imggray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "imgcanny=cv2.Canny(erode_image,94,255)\n",
    "contours,_ = cv2.findContours(imgcanny, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "rect_d=[] #store the dimensions of my bounding boxes\n",
    "min_cont_area=10\n",
    "for cnt in contours:\n",
    "    #print(cv2.contourArea(cnt))\n",
    "    if cv2.contourArea(cnt)>min_cont_area: #Limit the contours based on area?  \n",
    "        box_d=cv2.boundingRect(cnt)\n",
    "        x,y,w,h=box_d\n",
    "        rect_d.append([x,y,w,h]) \n",
    "        cv2.rectangle(imgCont, (x,y),(x+w,y+h), (0, 0, 255),1)\n",
    "\n",
    "i=0\n",
    "roi_n=[] #store each bounding box for later\n",
    "for d in range(0,len(rect_d)):    \n",
    "    x=rect_d[i][0] #Find x \n",
    "    y=rect_d[i][1] #Find y \n",
    "    w=rect_d[i][2] #Find width \n",
    "    h=rect_d[i][3] #Find height \n",
    "    roi=imgFinal[y:y+h,x:x+w]\n",
    "    roi_n.append(roi)\n",
    "    cv2.imwrite('Boxes/Boxed_ROIs_'+str(i) + '.png', roi)\n",
    "    i+=1\n",
    "\n",
    "cv2.imshow('Final Result',imgCont)\n",
    "cv2.imwrite('Final Result.jpg',imgCont)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a2f6aa-3975-446e-b183-d60c2237ed65",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzadanie.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      6\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m      7\u001b[0m blur \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "    \n",
    "image = cv2.imread(\"zadanie.png\")\n",
    "mask = np.zeros(image.shape, dtype=np.uint8)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,51,9)\n",
    "\n",
    "# Create horizontal kernel then dilate to connect text contours\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "# Find contours and filter out noise using contour approximation and area filtering\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    peri = cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, 0.04 * peri, True)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    area = w * h\n",
    "    ar = w / float(h)\n",
    "    if area > 1200 and area < 50000 and ar <8:\n",
    "        cv2.drawContours(mask, [c], -1, (255,255,255), -1)\n",
    "        \n",
    "# Bitwise-and input image and mask to get result\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "result = cv2.bitwise_and(image, image, mask=mask)\n",
    "result[mask==0] = (255,255,255) # Color background white\n",
    "\n",
    "# NEW CODE HERE TO END _____________________________________________________________\n",
    "gray2 = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "thresh2 = cv2.threshold(gray2, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "thresh2 = 255 - thresh2\n",
    "kernel = np.ones((5 ,191), np.uint8)\n",
    "close = cv2.morphologyEx(thresh2, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# get external contours\n",
    "contours = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "\n",
    "# draw contours\n",
    "result2 = result.copy()\n",
    "for cntr in contours:\n",
    "    # get bounding boxes\n",
    "    pad = 10\n",
    "    x,y,w,h = cv2.boundingRect(cntr)\n",
    "    cv2.rectangle( (x-pad, y-pad), (x+w+pad, y+h+pad), (0, 0, 255), 4)\n",
    "\n",
    "cv2.imwrite(\"john_bboxes.jpg\", result2)\n",
    "\n",
    "cv2.imshow(\"thresh\",thresh)\n",
    "cv2.imshow(\"dilate\",dilate)\n",
    "cv2.imshow(\"result\",result)\n",
    "cv2.imshow(\"gray2\",gray2)\n",
    "cv2.imshow(\"thresh2\",thresh2)\n",
    "cv2.imshow(\"close\",close)\n",
    "cv2.imshow(\"result2\",result2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74fc060a-ccb3-435c-a077-111c0d6b1a00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzadanie.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      7\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m      8\u001b[0m blur \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "image = cv2.imread(\"zadanie.png\")\n",
    "mask = np.zeros(image.shape, dtype=np.uint8)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 51, 9)\n",
    "\n",
    "# Create horizontal kernel then dilate to connect text contours\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "# Find contours and filter out noise using contour approximation and area filtering\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for i, c in enumerate(cnts):\n",
    "    peri = cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, 0.04 * peri, True)\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    area = w * h\n",
    "    ar = w / float(h)\n",
    "    if area > 1200 and area < 50000 and ar < 8:\n",
    "        cv2.drawContours(mask, [c], -1, (255, 255, 255), -1)\n",
    "        \n",
    "# Bitwise-and input image and mask to get result\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "result = cv2.bitwise_and(image, image, mask=mask)\n",
    "result[mask == 0] = (255, 255, 255)  # Color background white\n",
    "\n",
    "# New code\n",
    "gray2 = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "thresh2 = cv2.threshold(gray2, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "thresh2 = 255 - thresh2\n",
    "kernel = np.ones((5, 191), np.uint8)\n",
    "close = cv2.morphologyEx(thresh2, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Get external contours\n",
    "contours = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "\n",
    "# Draw contours and save each element\n",
    "result2 = result.copy()\n",
    "output_folder = \"output\"  # Folder to save the elements\n",
    "\n",
    "\n",
    "for  cntr in contours:\n",
    "    # Get bounding boxes\n",
    "    pad = 10\n",
    "    x, y, w, h = cv2.boundingRect(cntr)\n",
    "    cv2.rectangle(result2, (x - pad, y - pad), (x + w + pad, y + h + pad), (0, 0, 255), 4)\n",
    "    element = result[y - pad:y + h + pad, x - pad:x + w + pad]\n",
    "    cv2.imwrite(\"output/wynik\"+str(i)+\".png\", element)\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "cv2.imwrite(\"john_bboxes.jpg\", result2)\n",
    "\n",
    "cv2.imshow(\"thresh\",thresh)\n",
    "cv2.imshow(\"dilate\",dilate)\n",
    "cv2.imshow(\"result\",result)\n",
    "cv2.imshow(\"gray2\",gray2)\n",
    "cv2.imshow(\"thresh2\",thresh2)\n",
    "cv2.imshow(\"close\",close)\n",
    "cv2.imshow(\"result2\",result2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242a0ee-2fa8-4590-a52f-5929a94edad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# load the input image and grab the image dimensions\n",
    "image = cv2.imread(\"eroded_image.png\")\n",
    "orig = image.copy()\n",
    "(H, W) = image.shape[:2]\n",
    "# set the new width and height and then determine the ratio in change\n",
    "# for both the width and height\n",
    "(newW, newH) = (320,320)\n",
    "rW = W / float(newW)\n",
    "rH = H / float(newH)\n",
    "# resize the image and grab the new image dimensions\n",
    "image = cv2.resize(image, (newW, newH))\n",
    "(H, W) = image.shape[:2]\n",
    "# define the two output layer names for the EAST detector model that\n",
    "# we are interested -- the first is the output probabilities and the\n",
    "# second can be used to derive the bounding box coordinates of text\n",
    "layerNames = [\n",
    "\t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "\t\"feature_fusion/concat_3\"]\n",
    "\n",
    "# load the pre-trained EAST text detector\n",
    "print(\"[INFO] loading EAST text detector...\")\n",
    "net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "# construct a blob from the image and then perform a forward pass of\n",
    "# the model to obtain the two output layer sets\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "\t(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "start = time.time()\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layerNames)\n",
    "end = time.time()\n",
    "# show timing information on text prediction\n",
    "print(\"[INFO] text detection took {:.6f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126da932-f8df-484a-8598-e99676c1687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the number of rows and columns from the scores volume, then\n",
    "# initialize our set of bounding box rectangles and corresponding\n",
    "# confidence scores\n",
    "(numRows, numCols) = scores.shape[2:4]\n",
    "rects = []\n",
    "confidences = []\n",
    "# loop over the number of rows\n",
    "for y in range(0, numRows):\n",
    "\t# extract the scores (probabilities), followed by the geometrical\n",
    "\t# data used to derive potential bounding box coordinates that\n",
    "\t# surround text\n",
    "\tscoresData = scores[0, 0, y]\n",
    "\txData0 = geometry[0, 0, y]\n",
    "\txData1 = geometry[0, 1, y]\n",
    "\txData2 = geometry[0, 2, y]\n",
    "\txData3 = geometry[0, 3, y]\n",
    "\tanglesData = geometry[0, 4, y]\n",
    "    \n",
    "    # loop over the number of columns\n",
    "\tfor x in range(0, numCols):\n",
    "\t\t# if our score does not have sufficient probability, ignore it\n",
    "\t\tif scoresData[x] < 0.5:\n",
    "\t\t\tcontinue\n",
    "\t\t# compute the offset factor as our resulting feature maps will\n",
    "\t\t# be 4x smaller than the input image\n",
    "\t\t(offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\t\t# extract the rotation angle for the prediction and then\n",
    "\t\t# compute the sin and cosine\n",
    "\t\tangle = anglesData[x]\n",
    "\t\tcos = np.cos(angle)\n",
    "\t\tsin = np.sin(angle)\n",
    "\t\t# use the geometry volume to derive the width and height of\n",
    "\t\t# the bounding box\n",
    "\t\th = xData0[x] + xData2[x]\n",
    "\t\tw = xData1[x] + xData3[x]\n",
    "\t\t# compute both the starting and ending (x, y)-coordinates for\n",
    "\t\t# the text prediction bounding box\n",
    "\t\tendX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "\t\tendY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "\t\tstartX = int(endX - w)\n",
    "\t\tstartY = int(endY - h)\n",
    "\t\t# add the bounding box coordinates and probability score to\n",
    "\t\t# our respective lists\n",
    "\t\trects.append((startX, startY, endX, endY))\n",
    "\t\tconfidences.append(scoresData[x])\n",
    "# apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "# boxes\n",
    "boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "# loop over the bounding boxes\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "\t# scale the bounding box coordinates based on the respective\n",
    "\t# ratios\n",
    "\tstartX = int(startX * rW)\n",
    "\tstartY = int(startY * rH)\n",
    "\tendX = int(endX * rW)\n",
    "\tendY = int(endY * rH)\n",
    "\t# draw the bounding box on the image\n",
    "\tcv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Text Detection\", orig)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b828030-eb25-4673-bc6f-af912bdbacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importuj potrzebne biblioteki\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# załaduj model OCR do rozpoznawania pisma odręcznego\n",
    "print(\"[INFO] wczytywanie modelu OCR do rozpoznawania pisma odręcznego...\")\n",
    "model = tf.keras.models.load_model('Models/mymodel')\n",
    "# load the input image from disk, convert it to grayscale, and blur\n",
    "# it to reduce noise\n",
    "image = cv2.imread(\"zadanie.png\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "# perform edge detection, find contours in the edge map, and sort the\n",
    "# resulting contours from left-to-right\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
    "# initialize the list of contour bounding boxes and associated\n",
    "# characters that we'll be OCR'ing\n",
    "chars = []\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# compute the bounding box of the contour\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\t# filter out bounding boxes, ensuring they are neither too small\n",
    "\t# nor too large\n",
    "\tif (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n",
    "\t\t# extract the character and threshold it to make the character\n",
    "\t\t# appear as *white* (foreground) on a *black* background, then\n",
    "\t\t# grab the width and height of the thresholded image\n",
    "\t\troi = gray[y:y + h, x:x + w]\n",
    "\t\tthresh = cv2.threshold(roi, 0, 255,\n",
    "\t\t\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\t\t# if the width is greater than the height, resize along the\n",
    "\t\t# width dimension\n",
    "\t\tif tW > tH:\n",
    "\t\t\tthresh = imutils.resize(thresh, width=32)\n",
    "\t\t# otherwise, resize along the height\n",
    "\t\telse:\n",
    "\t\t\tthresh = imutils.resize(thresh, height=32)\n",
    "            # re-grab the image dimensions (now that its been resized)\n",
    "\t\t# and then determine how much we need to pad the width and\n",
    "\t\t# height such that our image will be 32x32\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\t\tdX = int(max(0, 32 - tW) / 2.0)\n",
    "\t\tdY = int(max(0, 32 - tH) / 2.0)\n",
    "\t\t# pad the image and force 32x32 dimensions\n",
    "\t\tpadded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n",
    "\t\t\tleft=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n",
    "\t\t\tvalue=(0, 0, 0))\n",
    "\t\tpadded = cv2.resize(padded, (32, 32))\n",
    "\t\t# prepare the padded image for classification via our\n",
    "\t\t# handwriting OCR model\n",
    "\t\tpadded = padded.astype(\"float32\") / 255.0\n",
    "\t\tpadded = np.expand_dims(padded, axis=-1)\n",
    "\t\t# update our list of characters that will be OCR'd\n",
    "\t\tchars.append((padded, (x, y, w, h)))\n",
    "        # OCR the characters using our handwriting recognition model\n",
    "preds = model.predict(chars)\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6d365e-d32d-4a92-a9e1-b88a8eceb14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELWAB SA\n",
      "UL. SMOLEŃSKA 64\n",
      "80-410 GDAŃSK\n",
      "Tel: 0483414002 Fax: 0483410350\n",
      "NIP. 684-001-04-59\n",
      "dn.14r12.21 wydr.1129\n",
      "PARAGON FISKALNY\n",
      "WODA MINERAL. 1,5L 3*2,00=6,00C\n",
      "PIZZA 1*5,00=5,00A ' 1\n",
      "JABŁKA 1,84 * 2,99=5,50 B\n",
      "RABAT 105 NA JABŁKA -0,55\n",
      "PODSUMA 1595 ) 2\n",
      "RAZEM DO OBNIŻKA ŚWIĄTECZNA 16,50\n",
      "RABAT OBNIŻKA ŚWIĄTECZNA 165 || 3\n",
      "RAZEM DO PROM. 3 WCENIE 2 6,00\n",
      "RABAT PROM. 3 W CENIE 2 -2,00\n",
      "Sp.op.A 4,50 PTU A=22,00% 0,81\n",
      "Sp.op.B 4,50 PTU B= 7,00% 029 [ 4\n",
      "Sp.op.C 3,40 PTU C= 0,00% 0,00\n",
      "Razem PTU 1,10\n",
      "SUMA PLN 12,30 |) 5\n",
      "ZAPŁACONO GOTÓWKĄ PLN 12,30\n",
      "0036/0041 0130 SZEF 9:48\n",
      "PL BAQ 00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "image_path =\"ilustracja_pytania.jpg\"\n",
    "# Konfiguracja pytesseracta\n",
    "custom_config = r'--oem 3 --psm 6 -l pol'\n",
    "# Możesz dostosować powyższą konfigurację w zależności od swoich potrzeb\n",
    "\n",
    "# Ścieżka do pliku obrazu z tekstem\n",
    "\n",
    "# Wczytanie obrazu\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Wykorzystanie pytesseracta do rozpoznania tekstu\n",
    "recognized_text = pytesseract.image_to_string(image, config=custom_config)\n",
    "\n",
    "# Wyświetlenie rozpoznanego tekstu\n",
    "print(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654129bf-ffb8-47d5-b444-008acd00d2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
